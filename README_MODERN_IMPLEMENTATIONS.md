# üöÄ Modern Neural Network Implementations: NumPy vs PyTorch vs TensorFlow

This project now includes **three different implementations** of the same MNIST digit classification neural network, showcasing the evolution from pure NumPy to modern deep learning frameworks.

## üìä Implementation Overview

| Implementation | File | Framework | Key Features |
|----------------|------|-----------|-------------|
| **Original** | `artificial_neural_network_MNIST.py` | NumPy | Pure Python, Educational |
| **PyTorch** | `artificial_neural_network_MNIST_pytorch.py` | PyTorch | GPU Support, Automatic Differentiation |
| **TensorFlow** | `artificial_neural_network_MNIST_tensorflow.py` | TensorFlow/Keras | High-level API, Production Ready |
| **Comparison** | `compare_all_implementations.py` | All Three | Performance Analysis |

## üèóÔ∏è Architecture Comparison

### All implementations share the same basic architecture:
- **Input Layer**: 784 neurons (28√ó28 pixels)
- **Hidden Layer**: 10 neurons with ReLU activation
- **Output Layer**: 10 neurons (digits 0-9)

### Key Differences:

#### 1. **NumPy Implementation** (Original)
```python
# Manual implementation of everything
def forward_prop(W1, b1, W2, b2, X):
    Z1 = W1 @ X + b1
    A1 = ReLU(Z1)
    Z2 = W2 @ A1 + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

def back_prop(Z1, A1, Z2, A2, W1, W2, X, Y):
    # Manual gradient calculations
    m = Y.shape[1]
    dZ2 = A2 - Y
    dW2 = 1 / m * dZ2 @ A1.T
    # ... more manual calculations
```

#### 2. **PyTorch Implementation**
```python
class MNISTNet(nn.Module):
    def __init__(self):
        super(MNISTNet, self).__init__()
        self.fc1 = nn.Linear(784, 10)
        self.fc2 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Automatic differentiation!
loss.backward()
optimizer.step()
```

#### 3. **TensorFlow Implementation**
```python
model = keras.Sequential([
    layers.Dense(10, activation='relu', input_shape=(784,)),
    layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='sgd',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# One line training!
model.fit(X_train, Y_train, epochs=500)
```

## üöÄ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Download MNIST Data
- Download `mnist_test.csv` from [Kaggle MNIST dataset](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)
- Place in project directory

### 3. Run Implementations

#### Original NumPy Version
```bash
python artificial_neural_network_MNIST.py
```

#### PyTorch Version
```bash
python artificial_neural_network_MNIST_pytorch.py
```

#### TensorFlow Version
```bash
python artificial_neural_network_MNIST_tensorflow.py
```

#### Compare All Three
```bash
python compare_all_implementations.py
```

## üìà Performance Comparison

### Expected Results:

| Metric | NumPy | PyTorch | TensorFlow |
|--------|-------|---------|------------|
| **Accuracy** | 85-90% | 87-92% | 88-93% |
| **Training Time** | 60-120s | 30-60s | 20-40s |
| **GPU Support** | ‚ùå | ‚úÖ | ‚úÖ |
| **Memory Usage** | Low | Medium | Medium |
| **Ease of Use** | Educational | Moderate | High |

### Why Different Performance?

1. **NumPy**: Pure Python loops, no optimization
2. **PyTorch**: Optimized C++ backend, GPU acceleration
3. **TensorFlow**: Highly optimized, graph compilation

## üî¨ Key Learning Differences

### 1. **Gradient Computation**

**NumPy** (Manual):
```python
dZ2 = A2 - Y
dW2 = 1 / m * dZ2 @ A1.T
db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)
```

**PyTorch** (Automatic):
```python
loss = criterion(outputs, targets)
loss.backward()  # Automatically computes all gradients!
```

**TensorFlow** (Automatic):
```python
# Gradients computed automatically during model.fit()
# Or manually with GradientTape for custom training
```

### 2. **GPU Acceleration**

**NumPy**: CPU only
```python
# All operations on CPU
data = np.array(data)
```

**PyTorch**: Easy GPU support
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
data = data.to(device)
```

**TensorFlow**: Automatic GPU detection
```python
# TensorFlow automatically uses GPU if available
# No code changes needed!
```

### 3. **Modern Features**

#### PyTorch Advantages:
- ‚úÖ Dynamic computation graphs
- ‚úÖ Pythonic and intuitive
- ‚úÖ Great for research
- ‚úÖ Excellent debugging

#### TensorFlow Advantages:
- ‚úÖ Production deployment
- ‚úÖ TensorBoard visualization
- ‚úÖ Mobile/web deployment
- ‚úÖ High-level Keras API

#### NumPy Advantages:
- ‚úÖ Complete understanding
- ‚úÖ No black boxes
- ‚úÖ Educational value
- ‚úÖ Minimal dependencies

## üõ†Ô∏è Advanced Features

### PyTorch Implementation Includes:
- **GPU acceleration**
- **Batch processing**
- **Model saving/loading**
- **Automatic differentiation**

### TensorFlow Implementation Includes:
- **Simple model** (matching original)
- **Advanced model** (multiple layers + dropout)
- **Training callbacks** (early stopping, learning rate reduction)
- **Training history visualization**

### Comparison Script Includes:
- **Side-by-side performance comparison**
- **Accuracy and speed benchmarks**
- **Visualization of results**
- **Parameter counting**

## üéØ Use Cases

### Choose **NumPy** when:
- Learning neural network fundamentals
- Understanding the math behind ML
- Educational purposes
- No GPU available

### Choose **PyTorch** when:
- Research and experimentation
- Dynamic model architectures
- Custom training loops needed
- Debugging complex models

### Choose **TensorFlow** when:
- Production deployment
- Standard architectures
- Team collaboration
- Mobile/web deployment needed

## üîß Customization Examples

### Modify Architecture
```python
# PyTorch
class CustomNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)  # Larger hidden layer
        self.fc2 = nn.Linear(128, 64)   # Additional layer
        self.fc3 = nn.Linear(64, 10)    # Output layer

# TensorFlow
model = keras.Sequential([
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])
```

### Add Regularization
```python
# PyTorch
class RegularizedNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.dropout = nn.Dropout(0.2)
        # ... rest of network

# TensorFlow
model.add(layers.Dropout(0.2))
```

## üìö Learning Path

1. **Start with NumPy** - Understand the fundamentals
2. **Move to PyTorch** - Learn modern deep learning
3. **Explore TensorFlow** - Understand production systems
4. **Compare all three** - See the trade-offs

## üèÜ Expected Improvements

Moving from NumPy ‚Üí PyTorch ‚Üí TensorFlow:

- **Speed**: 3-5x faster training
- **Accuracy**: 2-5% improvement
- **Ease of use**: Significantly easier
- **Scalability**: Much better for larger models
- **Debugging**: Better tools and error messages

## üîó Next Steps

After mastering these implementations:

1. **Try CNN architectures** for better MNIST performance
2. **Experiment with different optimizers** (Adam, RMSprop)
3. **Add data augmentation** for improved generalization
4. **Deploy models** using TensorFlow Serving or PyTorch TorchScript
5. **Scale to larger datasets** (CIFAR-10, ImageNet)

---

**üéâ You now have three different ways to solve the same problem - each teaching different aspects of modern machine learning!**
